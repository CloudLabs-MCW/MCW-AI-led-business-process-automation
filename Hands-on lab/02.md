## Exercise 2: Extract Health Analytics from visit audio records

Duration: 60 minutes

Contoso Healthcare hospitals upload audio recordings of patient visits to an Azure Storage Blob service. An Azure Function will be triggered with an Event Grid subscription/event handler to process recordings. The function will first detect the language of the recording using [Azure Cognitive Speech Audio Language Identification](https://docs.microsoft.com/azure/cognitive-services/speech-service/how-to-automatic-language-detection?pivots=programming-language-csharp) and then transcribe it to text. Once transcriptions are ready, Spanish records will be translated to English based on Contoso's requirements. Finally, [Azure Cognitive Services Text Analytics for Health](https://docs.microsoft.com/azure/cognitive-services/text-analytics/how-tos/text-analytics-for-health?tabs=ner) will extract and label relevant medical information to provide a richer search experience. During the exercise, you will integrate all the pieces, run a couple of sample recordings, and observe the results.

### Task 1: Configuring Azure Functions and Event Grid for audio uploads

As part of its automation process, Contoso will upload audio recordings of patient visits as WAV files to an Azure Storage account as blobs. An Azure Function App will detect new files and process them with multiple Cognitive Services. For the Functions App to detect new blobs, you will be using a new Azure Event Grid subscription and defining an event handler for the matching Azure Function.

1. In the [Azure portal](https://portal.azure.com), select your **contosoSUFFIX** Storage Account in the lab resource group.

   ![Lab resource group is open. The storage account is highlighted.](media/select-storage-account.png "Storage Account Selection")

2. From the left menu, select **Events (1)**. Make sure you are on the **Get Started (2)** page. Select **Azure Function (3)** as the event destination type. Select **Create (4)** to continue.

   ![Storage account page is open. The events panel is shown. Azure Functions is selected. Create button is highlighted.](media/storage-event-function.png "Create Storage Event Subscription")

3. Beneath the **Function Apps** header, expand the function app named **contoso-func-SUFFIX**. Expand the **Functions (Read Only)** item to get a list of functions available. From the functions list, select the **AudioProcessing** function.

   ![Function Apps are listed. Contoso function app functions are shown. ClaimsProcessing function is highlighted.](media/event-grid-select-audioprocessing.png "Function Selection for Event Grid")

4. Select **Add Event Grid Subscription (1)**.

   ![ClaimsProcessing function is selected. Add Event Grid Subscription link is highlighted.](media/event-grid-add-subscription-for-audioprocessing.png "Add Event Grid Subscription")

5. Set the values listed below.

    - **Name (1):** **AudioEvents**
    - **Topic Type (2):** Storage account.
    - **Source Resource (3):** Contoso storage account.
    - **System Topic Name (4):** **DocumentEvent**
    - **Filter to Event Types (5):** Blob Created

   ![Create event subscription page is presented. The event name is set to DocumentEvents. Topic Type is set to Storage account. Source Resource is set to contosoSUFFIX storage account. System Topic Name is set to DocumentEvent. Blob Created and Blob Deleted events are selected. Create button is highlighted.](media/event-grid-create-subscription-for-audio.png "Event Grid Subscription Settings")

    Select **Create (6)** to continue.

### Task 2: Connecting Cognitive Services to Azure Functions

For audio recording processing, the AudioProcessing function will use multiple Cognitive Service resources. Cognitive Services Speech will be used to detect the language of the recording and to transcribe the audio file. Cognitive Services Translator will be used to translate Spanish transcriptions to English. Finally, Cognitive Services Text Analytics will be used to extract and label relevant medical information from transcriptions. In this task, you will be connecting all the required Cognitive Services to the AudioProcessing Azure Function.

1. In the [Azure portal](https://portal.azure.com), select the **contoso-speech-SUFFIX** Cognitive Service from the lab resource group.

   ![Lab resource group is open. The Cognitive Service Speech resource is highlighted.](media/select-speech-resource.png "Cognitive Service Speech Resource Selection")

2. From the left menu, select **Keys and Endpoint (1)**. Copy **Key 1 (2)** and **Location (3)** to a text editor of your choice to be used later in the lab.

   ![Cognitive Services Speech Keys and Endpoint panel is shown. Key 1 Copy and Location Copy buttons are highlighted.](media/get-speech-service-keys.png "Copy Cognitive Service Speech Key and Endpoint")

3. Return to the lab resource group and select the Cognitive Service named **contoso-translate-SUFFIX** in your lab resource group.

   ![Lab resource group is open. The Cognitive Service Translate resource is highlighted.](media/select-translate-resource.png "Cognitive Service Translate Resource Selection")

4. From the left menu, select **Keys and Endpoint (1)**. Copy **Key 1 (2)**, **Location (3)** and **Text Translation Endpoint (4)** to a text editor of your choice to be used later in the lab.

   ![Cognitive Services Translate Keys and Endpoint panel is shown. Key 1 Copy, Location Copy, and Text Translation copy buttons are highlighted.](media/get-translate-service-keys.png "Copy Cognitive Service Translate Key and Endpoint")

5. Return to the lab resource group and select the Cognitive Service named **contoso-textanalytics-SUFFIX** in your lab resource group.

   ![Lab resource group is open. The Cognitive Service Text Analytics resource is highlighted.](media/select-textanalytics-resource.png "Cognitive Service Text Analytics Resource Selection")

6. From the left menu, select **Keys and Endpoint (1)**. Copy **Key 1 (2)** and **Endpoint (3)** to a text editor of your choice to be used later in the lab.

   ![Cognitive Services Text Analytics Keys and Endpoint panel is shown. Key 1 Copy and Endpoint copy buttons are highlighted.](media/get-textanalytics-service-keys.png "Copy Cognitive Service Text Analytics Key and Endpoint")

7. Return to the lab resource group and select the Function App named **contoso-func-SUFFIX** in your lab resource group.

   ![Resource group page is open. Function App is highlighted.](media/select-azure-function.png "Select Function App.")

8. From the left menu, select **Configuration (1)**, then select **New application setting (2)**.

   ![Function App Configuration page is open. New application setting link is highlighted.](media/function-app-new-application-setting-step2.png "Function App New Application Setting")

9. Set **Name (1)** to **SpeechRegion** and **Value (2)** to the previously copied Speech service's **Location**. Select **OK (3)** to save.

   ![Add Edit Application setting panel is open. Name is set to SpeechRegion. Value is set to the previously copied speech service region. The OK button is highlighted.](media/function-app-setting-speech-region.png)

10. Repeat the same steps to add the **Application Settings** listed below.

    | Name                  | Value                                                                                            |
    |-----------------------|--------------------------------------------------------------------------------------------------|
    | SpeechKey             | Previously copied **Key 1** for Cognitive Services Speech resource                               |
    | TranslatorKey         | Previously copied **Key 1** for Cognitive Services Text Translation resource                     |
    | TranslatorEndpoint    | Previously copied **Text Translation Endpoint** for Cognitive Services Text Translation resource |
    | TranslatorLocation    | Previously copied **Location** for Cognitive Services Text Translation resource                  |
    | TextAnalyticsKey      | Previously copied **Key 1** for Cognitive Services Text Analytics resource                       |
    | TextAnalyticsEndpoint | Previously copied **Endpoint** for Cognitive Services Text Analytics resource                    |

11. Once all settings **(1)** are set, select **Save (2)**.

    ![New application settings are highlighted. Save button is pointed.](media/function-app-settings-save-step2.png "Save new application settings")

12. Restart your Function App by selecting **Overview (1)** and **Restart (2)**.

   ![Restarting the Function App after configuring service integrations.](media/function-app-restart.png "Restarting Function App")

### Task 3: Implementing Cognitive Services for audio processing

In this task, we will look into the implementation of various Cognitive Services used to process patient audio recordings to detect the spoken language, transcribe, translate, and finally extract healthcare analytics.

1. Connect to your LABVM. Open **File Explorer** and navigate to the `C:\MCW\MCW-main\Hands-on lab\lab-files\source-azure-functions\Lab-DocumentProcessing` folder. Open **DocumentProcessing** solution file.

    ![File Explorer shows the DocumentProcessing folder in C:\MCW\MCW-main\Hands-on lab\lab-files\source-azure-functions\Lab-DocumentProcessing. DocumentProcessing solution file is highlighted.](media/visualstudio-open-documentprocessing.png "DocumentProcessing Solution")

2. Once the solution is open, select the **AudioRecording.cs (1)** file from the Solution Explorer. Analyze the code that starts with **Audio Language Identification** comment. The WAV file for the recording is downloaded to a temporary local storage and passed to **SourceLanguageRecognizer**. The result is the detected language from the set of languages passed in the **autoDetectSourceLanguageConfig** object.

   ![DocumentProcessing solution is open in Visual Studio. AudioRecording.cs is shown. Audio Language Identification code is highlighted.](media/visualstudio-audiorecording.png "AudioRecording CS File")

3. Scrolling down the **AudioRecording.cs (1)** file, you can find the **Audio Transcription** comment line where the transcription code starts. In this case, **SpeechRecognizer** is used to transcribe the audio file. The recognizer returns results as it goes. The code combines the **RecognizedSpeech** using a StringBuilder.

   ```cs
   // Audio Transcription
   StringBuilder sb = new StringBuilder();
   using var audioConfig = AudioConfig.FromWavFileInput(tempPath);
   {
       using var recognizer = new SpeechRecognizer(speechConfig, audioConfig);
       {
           var stopRecognition = new TaskCompletionSource<int>();
           recognizer.SessionStopped += (s, e) =>
           {
               stopRecognition.TrySetResult(0);
           };
           recognizer.Canceled += (s, e) =>
           {
               stopRecognition.TrySetResult(0);
           };
           recognizer.Recognized += (s, e) =>
           {
               if (e.Result.Reason == ResultReason.RecognizedSpeech)
               {
                   sb.Append(e.Result.Text);
               }
               else if (e.Result.Reason == ResultReason.NoMatch)
               {
                   log.LogInformation($"NOMATCH: Speech could not be recognized.");
               }
           };
           await recognizer.StartContinuousRecognitionAsync();
           Task.WaitAny(new[] { stopRecognition.Task });
       }
   }
   string transcribedText = sb.ToString();
   ```

4. Once the transcription process is complete, it is time to translate Spanish transcriptions into English. In this case, we are sending an HTTP request with the proper headers to the Translator endpoint and get back the translated document.

   ```cs
   // If transcription is in Spanish we will translate it to English
   if (!languageDetected.Contains("en"))
   {
       string route = $"/translate?api-version=3.0&to=en";
       string textToTranslate = sb.ToString();
       object[] body = new object[] { new { Text = textToTranslate } };
       var requestBody = JsonConvert.SerializeObject(bo
       using (var client = new HttpClient())
       using (var request = new HttpRequestMessage())
       {
           request.Method = HttpMethod.Post;
           request.RequestUri = new Uri(translatorEndpoint + route);
           request.Content = new StringContent(requestBody, Encoding.UTF8, "application/json");
           request.Headers.Add("Ocp-Apim-Subscription-Key", translatorKey);
           request.Headers.Add("Ocp-Apim-Subscription-Region", translatorLocati
           HttpResponseMessage response = await client.SendAsync(request).ConfigureAwait(false);
           var responseBody = await response.Content.ReadAsStringAsync();
           List<Model.TranslatorService.Root> translatedDocuments = JsonConvert.DeserializeObject<List<Model.TranslatorService.Root>>(responseBody);
           transcribedText = translatedDocuments.FirstOrDefault().Translations.FirstOrDefault().Text;
       }
   }
   ```

5. Scrolling down, you will find a TODO comment line that says **//TODO:Azure Text Analytics for Healthcare**. Now it is your turn to implement Text Analytics for Healthcare. Copy and paste the code below to complete the implementation.

   ```cs
   List<string> healthDocuments = new List<string>
   {
       transcribedText
   };
   var textAnalyticsClient = new TextAnalyticsClient(new Uri(textAnalyticsEndpoint), new AzureKeyCredential(textAnalyticsKey));
   AnalyzeHealthcareEntitiesOperation healthOperation = textAnalyticsClient.StartAnalyzeHealthcareEntities(healthDocuments, "en", new AnalyzeHealthcareEntitiesOptions { });
   await healthOperation.WaitForCompletionAsync();
   AnalyzeHealthcareEntitiesResult healthcareResult = healthOperation.GetValues().FirstOrDefault().FirstOrDefault();
   ```

   Here you can see that a TextAnalyticsClient is being created out of the service endpoint and credentials. **StartAnalyzeHealthcareEntities** method starts the processing for the set of documents provided. As we are passing only a single document at a time we pull out the first document out of the result set.

6. Now you can close Visual Studio. You don't have to worry about the changes you have implemented. A fully functional version of the Function App is already deployed to your Lab environment and will be soon ready to be tested.

### Task 4: Running audio record processing automation

Now that all implementations are completed, we can upload new patient recordings and see the entire process of transcription, translation, and the extraction of medical information from audio files.

1. Connect to your LABVM. Open **Edge** and navigate to the [Azure portal](https://portal.azure.com). Enter your credentials to access your subscriptions. Navigate to **contosoSUFFIX** storage account in the lab resource group.

    ![Edge is highlighted on the desktop. Browser is open and navigated to portal.azure.com. Storage account overview page is open.](media/azure-portal-labvm.png "Storage Account on Lab VM")

2. From the left menu, select **Containers (1)**, then select the **audiorecordings (2)** container.

   ![Contoso storage containers are listed. audiorecordings container is highlighted.](media/storage-audiorecordings-container.png "audiorecordings Storage Container")

3. Select **Upload (1)** and **Browse (2)**. Navigate to `C:\MCW\MCW-main\Hands-on lab\lab-files\audio-recordings` **(3)**. Choose all the files **(4)** and select **Open (5)**.

    ![Container page is open. The upload button is selected. File open dialog shows the audio-recordings folder with WAV files listed. All WAV files and Open button are highlighted.](media/upload-audio-recordings.png "Local file selection for upload.")

4. Select **Upload** to start the upload process.

5. The Azure Function for audio file processing will trigger for the files added. We should see the results of the audio transcription in Cosmos DB. Go back to your resource group and select your Cosmos DB resource named `contoso-cdb-SUFFIX`.

    ![Resource group page is open. CosmosDB service is highlighted.](media/select-cosmosdb-service.png "Select Cosmos DB service.")

6. Select **Data Explorer**.

    ![Cosmos DB Overview page is open. Data explorer button is highlighted.](media/cosmosdb-data-explorer.png "Cosmos DB Data Explorer")

7. Select the **Items (1)** list under the **Contoso** database's **Transcriptions** collection. Select the first document **(2)** to see its content. Take a look at the **TranscribedText (3)** and **HealthcareEntities (4)** extracted.

    > **Note:** It can take up to a minute for the initial results to show up. Refresh the collection every fifteen seconds to see the latest.

    ![Cosmos DB Data Explorer is open. A Transcriptions Document is shown in JSON format from the Transcriptions collection in the Contoso database.](media/cosmosdb-data-explorer-audio-transcriptions.png "Transcriptions Document in Cosmos DB")
